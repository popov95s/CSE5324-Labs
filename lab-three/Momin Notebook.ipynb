{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "The dataset we chose contains car evaluation data derived from a hierarchical decision model developed initially for a demonstration of a decision making model and can be found at https://archive.ics.uci.edu/ml/datasets/Car+Evaluation. The authors The dataset contains 6 attributes related to either price or technical characteristics. The 7th attribute represents the estimated class of the car and is based on all other attributes. The dataset consists of 1728 entries and is stripped of structural attributes, which means all attributes are directly related to the estimated car class attribute. There are also three intermediate attributes – PRICE, TECH and COMFORT – which are related to the 6 main attributes.  \n",
    "## Use case\n",
    "Choosing a vehicle to purchase can be a tedious process that involves hours of research, with studies showing American drivers spend an average of around 15 hours between realizing the need for a new car and making the purchase [1]. 60% of this time is usually spent in online research of specifications and availability. Generally, although the most important attribute of a car is its ability to transport, the final decision is very often based on an amalgam of its price, safety and capacity. The main purpose for the collection of the dataset we chose was to “actively support the decision maker in the knowledge acquisition and evaluation stages of the decision making process” [2]. \n",
    "## Prediction task\n",
    "The dataset uses a simple hierarchical model to classify cars in one of 4 categories: Unacceptable (unacc), Acceptable (acc), Good (good), Very Good (vgood). The criteria tree is displayed below. The goal of our prediction task is to correctly identify the class associated with the car based on the 6 attributes that are used in the evaluation model, without specifying the model structure itself. \n",
    "This could be useful in many different scenarios, such as online-based automotive research and shopping websites. Those often imply mathematical algorithms to present users with a nominal rating ranging \n",
    "\n",
    " \n",
    "\n",
    "<img src='tree.png' label=\"Criteria tree\"/ height=500 width=500>\n",
    "https://www.elephant.com/blog/car-insurance/new-study-details-how-long-it-takes-before-car-shoppers-buy \n",
    "http://kt.ijs.si/MarkoBohanec/pub/Avignon88.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "df = pd.read_csv('car.data')\n",
    "buying_maint_map = {'vhigh':3,'high':2,'med':1,'low':0}\n",
    "df['buy_price'] = df['buy_price'].map(buying_maint_map).astype(np.int)\n",
    "df['maint_price'] = df['maint_price'].map(buying_maint_map).astype(np.int)\n",
    "doors_map = {'2':0,'3':1,'4':2,'5more':3}\n",
    "df['doors'] = df['doors'].map(doors_map).astype(np.int)\n",
    "persons_map = {'2':0,'3':1,'4':2,'more':3}\n",
    "df['persons'] = df['persons'].map(persons_map).astype(np.int)\n",
    "trunk_map = {'small':0,'med':1,'big':2}\n",
    "df['trunk_size'] = df['trunk_size'].map(trunk_map).astype(np.int)\n",
    "safety_map = {'low':0,'med':1,'high':2}\n",
    "df['safety'] = df['safety'].map(safety_map).astype(np.int)\n",
    "class_map = {'unacc':0,'acc':1,'good':2,'vgood':3}\n",
    "df['class'] = df['class'].map(class_map).astype(np.int)\n",
    "\n",
    "\n",
    "feature_cols = ['buy_price','maint_price','doors','persons','trunk_size','safety']\n",
    "class_cols = ['class']\n",
    "\n",
    "#Make X a 2D numpy array\n",
    "X = df[feature_cols].as_matrix()\n",
    "#Make y a 1D numpy array\n",
    "y = (df[class_cols]==0).astype(np.int).values.ravel()\n",
    "y_not_binary = (df[class_cols]).astype(np.int).values.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from notebook 6\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001,l1_norm=0.5):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        if ((l1_norm > 1 or l1_norm < 0) and l1_norm != -1):\n",
    "            raise ValueError(\"L1 Norm must be between 0 and 1 or -1\")\n",
    "        self.l1_norm = l1_norm\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if (self.l1_norm == -1):\n",
    "            return gradient\n",
    "        return self._regularize(gradient)\n",
    "    \n",
    "    def _regularize(self,gradient):\n",
    "        #Implementation for Elastic Net regularization \n",
    "        sub_1 = np.copy(gradient[1:])\n",
    "        sub_2 = np.copy(gradient[1:])\n",
    "        \n",
    "        #Calculate L1 Norm \n",
    "        mask = np.logical_and(sub_1 >= (-self.C/2),sub_1 <= (self.C/2))\n",
    "        sub_1[mask] = 0\n",
    "        sub_1[sub_1 < (-self.C/2)] += (self.C / 2)\n",
    "        sub_1[sub_1 > (self.C/2)] -= (self.C / 2)\n",
    "        \n",
    "        \n",
    "        #Calculate L2 Norm\n",
    "        sub_2 += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        #Combine the regularizations to make an elastic net.\n",
    "        gradient[1:] = self.l1_norm * sub_1 + (1-self.l1_norm) * sub_2\n",
    "        return gradient\n",
    "        \n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.coursera.org/learn/ml-regression/lecture/SeZsT/coordinate-descent-for-lasso-normalized-features\n",
    "Coursera link used to help derive the formula for L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[ 3.10526841]\n",
      " [ 1.26327965]\n",
      " [ 1.14966388]\n",
      " [ 0.0206099 ]\n",
      " [-1.26879973]\n",
      " [-0.39749817]\n",
      " [-2.11235392]]\n",
      "Accuracy of:  0.8651620370370371\n",
      "Wall time: 50.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if (self.l1_norm == -1):\n",
    "            return gradient\n",
    "        return self._regularize(gradient)\n",
    "    \n",
    "    \n",
    "slr = StochasticLogisticRegression(0.1,1000, C=0.001,l1_norm=1) # take a lot more steps!!\n",
    "\n",
    "slr.fit(X,y)\n",
    "\n",
    "yhat = slr.predict(X)\n",
    "print(slr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[ 1.38419198e-03]\n",
      " [ 1.93458401e-04]\n",
      " [ 1.72030526e-04]\n",
      " [-5.93905279e-05]\n",
      " [-3.67708094e-04]\n",
      " [-1.65594500e-04]\n",
      " [-6.01517508e-04]]\n",
      "Accuracy of:  0.8929398148148148\n",
      "Wall time: 178 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class NewtonLogisticRegression(BinaryLogisticRegression):\n",
    "    def _hessian(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel()\n",
    "        return X.T @ np.diag(g*(1-g)) @ X - 2 * self.C \n",
    "    def fit(self,X,y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            inv_hessian = np.linalg.inv(self._hessian(Xb,y))\n",
    "            self.w_ +=  inv_hessian@gradient*self.eta # multiply by learning rate \n",
    "            \n",
    "nlr = NewtonLogisticRegression(0.1,10, C=0.001) # take a lot more steps!!\n",
    "nlr.fit(X,y)\n",
    "\n",
    "yhat = nlr.predict(X)\n",
    "print(nlr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.0001, optimize_func='steepest',l1_norm=0.5):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        self.optimize_func = optimize_func\n",
    "        self.l1_norm = l1_norm\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = (y==yval).astype(np.int) # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            if self.optimize_func == 'stochastic':\n",
    "                 hblr = StochasticLogisticRegression(self.eta,self.iters,self.C,l1_norm = self.l1_norm)\n",
    "            elif self.optimize_func == 'steepest':\n",
    "                hblr = BinaryLogisticRegression(self.eta,self.iters,self.C,l1_norm = self.l1_norm)\n",
    "            elif self.optimize_func == 'newton':\n",
    "                hblr = NewtonLogisticRegression(self.eta,self.iters,self.C,l1_norm = self.l1_norm)\n",
    "            hblr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ 1.07241361  0.86258341  0.80408431  0.14023905 -0.82246482 -0.0731898\n",
      "  -1.21356912]\n",
      " [-1.16057672 -0.36071249 -0.37714207 -0.18977163  0.4727244  -0.15508043\n",
      "   0.64756361]\n",
      " [-0.52221446 -0.99670015 -0.99670015 -0.26945457  0.11810753 -0.27441964\n",
      "   0.00443686]\n",
      " [-0.70688151 -1.02877499 -0.80201932 -0.2993838  -0.02292801 -0.02129836\n",
      "   0.22804441]]\n",
      "Accuracy of:  0.7644675925925926\n",
      "Wall time: 232 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(0.1,iterations=250,C=0.00001,optimize_func='steepest')\n",
    "lr.fit(X,y_not_binary)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y_not_binary,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ 3.43376064  0.91109925  1.05825449  0.14149201 -1.64407873 -0.27249408\n",
      "  -1.98629881]\n",
      " [-3.13606038 -0.41059259 -0.49080561 -0.01677221  0.75234871  0.15536722\n",
      "   0.82116911]\n",
      " [-1.19891464 -1.38832865 -1.82712731 -0.34378097  0.20434965 -0.66132599\n",
      "   0.63820601]\n",
      " [-1.81059544 -1.821184   -1.34306785 -0.57224608 -0.10109098  0.11113448\n",
      "   0.63007493]]\n",
      "Accuracy of:  0.78125\n",
      "Wall time: 147 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr_st = MultiClassLogisticRegression(0.1,iterations=1000,C=0.00001,optimize_func='stochastic')\n",
    "lr_st.fit(X,y_not_binary)\n",
    "print(lr_st)\n",
    "\n",
    "yhat_st = lr_st.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y_not_binary,yhat_st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ 2.76030519e-04  3.87913330e-05  3.45047708e-05 -1.17901003e-05\n",
      "  -7.34861502e-05 -3.29528930e-05 -1.20157500e-04]\n",
      " [-3.06442836e-04 -6.21252737e-06 -7.49849601e-06  7.07581525e-06\n",
      "   5.40827949e-05  1.56727174e-05  8.19803680e-05]\n",
      " [-1.92044544e-04 -1.72496597e-05 -1.72496597e-05  9.68229429e-07\n",
      "   9.64878935e-06  1.20559365e-06  1.20559365e-05]\n",
      " [-2.40568032e-04 -1.53205723e-05 -9.74804155e-06  3.75462921e-06\n",
      "   9.76375201e-06  1.60745820e-05  2.61211957e-05]]\n",
      "Accuracy of:  0.7864583333333334\n",
      "Wall time: 132 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(0.1,iterations=2,C=0.01,optimize_func='newton',l1_norm=0)\n",
    "lr.fit(X,y_not_binary)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y_not_binary,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "c_list = [0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "optimization_list = ['newton','stochastic','steepest']\n",
    "regularization_list = [-1,0,1]\n",
    "eta = 0.1\n",
    "\n",
    "iterations = {\n",
    "    'newton':3,\n",
    "    'stochastic':1000,\n",
    "    'steepest':1000\n",
    "}\n",
    "\n",
    "tests = dict.fromkeys(regularization_list,dict.fromkeys(optimization_list,dict.fromkeys(c_list,\"\")))\n",
    "results = copy.deepcopy(tests)\n",
    "weights = copy.deepcopy(tests)\n",
    "\n",
    "\n",
    "for regularization in tests.keys():\n",
    "    for optimization in tests[regularization].keys():\n",
    "        for c in tests[regularization][optimization].keys():\n",
    "            tests[regularization][optimization][c] = MultiClassLogisticRegression(eta,iterations=iterations[optimization],C=int(c),optimize_func=optimization,l1_norm=int(regularization))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 , newton , 1e-05 : Accuracy of:  0.7841435185185185\n",
      "-1 , newton , 0.0001 : Accuracy of:  0.7841435185185185\n",
      "-1 , newton , 0.001 : Accuracy of:  0.7841435185185185\n",
      "-1 , newton , 0.01 : Accuracy of:  0.7841435185185185\n",
      "-1 , newton , 0.1 : Accuracy of:  0.7841435185185185\n",
      "-1 , newton , 1 : Accuracy of:  0.7002314814814815\n",
      "-1 , newton , 10 : Accuracy of:  0.7002314814814815\n",
      "-1 , newton , 100 : Accuracy of:  0.7002314814814815\n",
      "-1 , newton , 1000 : Accuracy of:  0.7002314814814815\n",
      "\n",
      "\n",
      "-1 , stochastic , 1e-05 : Accuracy of:  0.7841435185185185\n",
      "-1 , stochastic , 0.0001 : Accuracy of:  0.7841435185185185\n",
      "-1 , stochastic , 0.001 : Accuracy of:  0.7841435185185185\n",
      "-1 , stochastic , 0.01 : Accuracy of:  0.7841435185185185\n",
      "-1 , stochastic , 0.1 : Accuracy of:  0.7841435185185185\n",
      "-1 , stochastic , 1 : Accuracy of:  0.7002314814814815\n",
      "-1 , stochastic , 10 : Accuracy of:  0.7002314814814815\n",
      "-1 , stochastic , 100 : Accuracy of:  0.7002314814814815\n",
      "-1 , stochastic , 1000 : Accuracy of:  0.7002314814814815\n",
      "\n",
      "\n",
      "-1 , steepest , 1e-05 : Accuracy of:  0.7841435185185185\n",
      "-1 , steepest , 0.0001 : Accuracy of:  0.7841435185185185\n",
      "-1 , steepest , 0.001 : Accuracy of:  0.7841435185185185\n",
      "-1 , steepest , 0.01 : Accuracy of:  0.7841435185185185\n",
      "-1 , steepest , 0.1 : Accuracy of:  0.7841435185185185\n",
      "-1 , steepest , 1 : Accuracy of:  0.7002314814814815\n",
      "-1 , steepest , 10 : Accuracy of:  0.7002314814814815\n",
      "-1 , steepest , 100 : Accuracy of:  0.7002314814814815\n",
      "-1 , steepest , 1000 : Accuracy of:  0.7002314814814815\n",
      "\n",
      "\n",
      "0 , newton , 1e-05 : Accuracy of:  0.7841435185185185\n",
      "0 , newton , 0.0001 : Accuracy of:  0.7841435185185185\n",
      "0 , newton , 0.001 : Accuracy of:  0.7841435185185185\n",
      "0 , newton , 0.01 : Accuracy of:  0.7841435185185185\n",
      "0 , newton , 0.1 : Accuracy of:  0.7841435185185185\n",
      "0 , newton , 1 : Accuracy of:  0.7002314814814815\n",
      "0 , newton , 10 : Accuracy of:  0.7002314814814815\n",
      "0 , newton , 100 : Accuracy of:  0.7002314814814815\n",
      "0 , newton , 1000 : Accuracy of:  0.7002314814814815\n",
      "\n",
      "\n",
      "0 , stochastic , 1e-05 : Accuracy of:  0.7841435185185185\n",
      "0 , stochastic , 0.0001 : Accuracy of:  0.7841435185185185\n",
      "0 , stochastic , 0.001 : Accuracy of:  0.7841435185185185\n",
      "0 , stochastic , 0.01 : Accuracy of:  0.7841435185185185\n",
      "0 , stochastic , 0.1 : Accuracy of:  0.7841435185185185\n",
      "0 , stochastic , 1 : Accuracy of:  0.7002314814814815\n",
      "0 , stochastic , 10 : Accuracy of:  0.7002314814814815\n",
      "0 , stochastic , 100 : Accuracy of:  0.7002314814814815\n",
      "0 , stochastic , 1000 : Accuracy of:  0.7002314814814815\n",
      "\n",
      "\n",
      "0 , steepest , 1e-05 : Accuracy of:  0.7841435185185185\n",
      "0 , steepest , 0.0001 : Accuracy of:  0.7841435185185185\n",
      "0 , steepest , 0.001 : Accuracy of:  0.7841435185185185\n",
      "0 , steepest , 0.01 : Accuracy of:  0.7841435185185185\n",
      "0 , steepest , 0.1 : Accuracy of:  0.7841435185185185\n",
      "0 , steepest , 1 : Accuracy of:  0.7002314814814815\n",
      "0 , steepest , 10 : Accuracy of:  0.7002314814814815\n",
      "0 , steepest , 100 : Accuracy of:  0.7002314814814815\n",
      "0 , steepest , 1000 : Accuracy of:  0.7002314814814815\n",
      "\n",
      "\n",
      "1 , newton , 1e-05 : Accuracy of:  0.7841435185185185\n",
      "1 , newton , 0.0001 : Accuracy of:  0.7841435185185185\n",
      "1 , newton , 0.001 : Accuracy of:  0.7841435185185185\n",
      "1 , newton , 0.01 : Accuracy of:  0.7841435185185185\n",
      "1 , newton , 0.1 : Accuracy of:  0.7841435185185185\n",
      "1 , newton , 1 : Accuracy of:  0.7002314814814815\n",
      "1 , newton , 10 : Accuracy of:  0.7002314814814815\n",
      "1 , newton , 100 : Accuracy of:  0.7002314814814815\n",
      "1 , newton , 1000 : Accuracy of:  0.7002314814814815\n",
      "\n",
      "\n",
      "1 , stochastic , 1e-05 : Accuracy of:  0.7841435185185185\n",
      "1 , stochastic , 0.0001 : Accuracy of:  0.7841435185185185\n",
      "1 , stochastic , 0.001 : Accuracy of:  0.7841435185185185\n",
      "1 , stochastic , 0.01 : Accuracy of:  0.7841435185185185\n",
      "1 , stochastic , 0.1 : Accuracy of:  0.7841435185185185\n",
      "1 , stochastic , 1 : Accuracy of:  0.7002314814814815\n",
      "1 , stochastic , 10 : Accuracy of:  0.7002314814814815\n",
      "1 , stochastic , 100 : Accuracy of:  0.7002314814814815\n",
      "1 , stochastic , 1000 : Accuracy of:  0.7002314814814815\n",
      "\n",
      "\n",
      "1 , steepest , 1e-05 : Accuracy of:  0.7841435185185185\n",
      "1 , steepest , 0.0001 : Accuracy of:  0.7841435185185185\n",
      "1 , steepest , 0.001 : Accuracy of:  0.7841435185185185\n",
      "1 , steepest , 0.01 : Accuracy of:  0.7841435185185185\n",
      "1 , steepest , 0.1 : Accuracy of:  0.7841435185185185\n",
      "1 , steepest , 1 : Accuracy of:  0.7002314814814815\n",
      "1 , steepest , 10 : Accuracy of:  0.7002314814814815\n",
      "1 , steepest , 100 : Accuracy of:  0.7002314814814815\n",
      "1 , steepest , 1000 : Accuracy of:  0.7002314814814815\n",
      "\n",
      "\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print\n",
    "for regularization in tests.keys():\n",
    "    for optimization in tests[regularization].keys():\n",
    "        for c in tests[regularization][optimization].keys():\n",
    "            test = tests[regularization][optimization][c]\n",
    "            test.fit(X,y_not_binary)\n",
    "            #print(test)\n",
    "            yhat = test.predict(X)\n",
    "            score = accuracy_score(y_not_binary,yhat)\n",
    "            print(regularization,',',optimization,',',c,': Accuracy of: ',score)\n",
    "            results[regularization][optimization][c] = score\n",
    "            weights[regularization][optimization][c] = test.w_\n",
    "        print()\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
