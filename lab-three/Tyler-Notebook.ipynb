{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 0 0]\n",
      "[0 0 0 ... 0 2 3]\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "df = pd.read_csv('car.data')\n",
    "buying_maint_map = {'vhigh':3,'high':2,'med':1,'low':0}\n",
    "df['buy_price'] = df['buy_price'].map(buying_maint_map).astype(np.int)\n",
    "df['maint_price'] = df['maint_price'].map(buying_maint_map).astype(np.int)\n",
    "doors_map = {'2':0,'3':1,'4':2,'5more':3}\n",
    "df['doors'] = df['doors'].map(doors_map).astype(np.int)\n",
    "persons_map = {'2':0,'3':1,'4':2,'more':3}\n",
    "df['persons'] = df['persons'].map(persons_map).astype(np.int)\n",
    "trunk_map = {'small':0,'med':1,'big':2}\n",
    "df['trunk_size'] = df['trunk_size'].map(trunk_map).astype(np.int)\n",
    "safety_map = {'low':0,'med':1,'high':2}\n",
    "df['safety'] = df['safety'].map(safety_map).astype(np.int)\n",
    "class_map = {'unacc':0,'acc':1,'good':2,'vgood':3}\n",
    "df['class'] = df['class'].map(class_map).astype(np.int)\n",
    "\n",
    "\n",
    "feature_cols = ['buy_price','maint_price','doors','persons','trunk_size','safety']\n",
    "class_cols = ['class']\n",
    "\n",
    "#Make X a 2D numpy array\n",
    "X = df[feature_cols].as_matrix()\n",
    "#Make y a 1D numpy array\n",
    "y = (df[class_cols]==0).astype(np.int).values.ravel()\n",
    "y_not_binary = (df[class_cols]).astype(np.int).values.ravel()\n",
    "print(y)\n",
    "print(y_not_binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 µs, sys: 14 µs, total: 26 µs\n",
      "Wall time: 27.7 µs\n"
     ]
    }
   ],
   "source": [
    "#Taken from notebook 6\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[ 3.25120051]\n",
      " [ 1.01238084]\n",
      " [ 1.19489056]\n",
      " [-0.39698096]\n",
      " [-0.8734036 ]\n",
      " [-0.15402843]\n",
      " [-1.94371543]]\n",
      "Accuracy of:  0.8310185185185185\n",
      "CPU times: user 22.3 ms, sys: 2.75 ms, total: 25 ms\n",
      "Wall time: 18.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "slr = StochasticLogisticRegression(0.1,1000, C=0.001) # take a lot more steps!!\n",
    "\n",
    "slr.fit(X,y)\n",
    "\n",
    "yhat = slr.predict(X)\n",
    "print(slr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.0001, optimize_func='steepest'):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        self.optimize_func = optimize_func\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = (y==yval).astype(np.int) # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            if self.optimize_func == 'stochastic':\n",
    "                 hblr = StochasticLogisticRegression(self.eta,self.iters,self.C)\n",
    "            else:\n",
    "                hblr = BinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            hblr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ 0.08093106  0.18241802  0.17404447  0.08358793 -0.05967811  0.03953186\n",
      "  -0.05569466]\n",
      " [-0.10943317 -0.15751531 -0.16004672 -0.13135618 -0.02709511 -0.0853471\n",
      "  -0.01265203]\n",
      " [-0.17085057 -0.27019536 -0.27019536 -0.23320433 -0.23126378 -0.16183375\n",
      "  -0.14974222]\n",
      " [-0.17436727 -0.27103951 -0.25973252 -0.23233586 -0.23616838 -0.14860788\n",
      "  -0.13741636]]\n",
      "Accuracy of:  0.7002314814814815\n",
      "CPU times: user 24.4 ms, sys: 35.5 ms, total: 59.9 ms\n",
      "Wall time: 10.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(0.1,iterations=8,C=0.00001)\n",
    "lr.fit(X,y_not_binary)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y_not_binary,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ 0.13789624  0.50501539  0.52283529  0.12388146 -0.70758088 -0.12445908\n",
      "  -0.76355279]\n",
      " [-0.30994299 -0.27381671 -0.37317727 -0.60701586  0.17559654 -0.13918127\n",
      "  -0.00809579]\n",
      " [-0.33305699 -0.49879457 -0.5052283  -0.28298403 -0.29364128 -0.20863712\n",
      "  -0.43610951]\n",
      " [-0.37428016 -0.54591627 -0.49269169 -0.53820893 -0.51189588 -0.35464699\n",
      "  -0.33945282]]\n",
      "Accuracy of:  0.7141203703703703\n",
      "CPU times: user 11.9 ms, sys: 7.87 ms, total: 19.8 ms\n",
      "Wall time: 8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr_st = MultiClassLogisticRegression(0.1,iterations=50,C=0.00001,optimize_func='stochastic')\n",
    "lr_st.fit(X,y_not_binary)\n",
    "print(lr_st)\n",
    "\n",
    "yhat_st = lr_st.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y_not_binary,yhat_st))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
