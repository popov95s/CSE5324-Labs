{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "df = pd.read_csv('car.data')\n",
    "buying_maint_map = {'vhigh':3,'high':2,'med':1,'low':0}\n",
    "df['buy_price'] = df['buy_price'].map(buying_maint_map).astype(np.int)\n",
    "df['maint_price'] = df['maint_price'].map(buying_maint_map).astype(np.int)\n",
    "doors_map = {'2':0,'3':1,'4':2,'5more':3}\n",
    "df['doors'] = df['doors'].map(doors_map).astype(np.int)\n",
    "persons_map = {'2':0,'3':1,'4':2,'more':3}\n",
    "df['persons'] = df['persons'].map(persons_map).astype(np.int)\n",
    "trunk_map = {'small':0,'med':1,'big':2}\n",
    "df['trunk_size'] = df['trunk_size'].map(trunk_map).astype(np.int)\n",
    "safety_map = {'low':0,'med':1,'high':2}\n",
    "df['safety'] = df['safety'].map(safety_map).astype(np.int)\n",
    "class_map = {'unacc':0,'acc':1,'good':2,'vgood':3}\n",
    "df['class'] = df['class'].map(class_map).astype(np.int)\n",
    "\n",
    "\n",
    "feature_cols = ['buy_price','maint_price','doors','persons','trunk_size','safety']\n",
    "class_cols = ['class']\n",
    "\n",
    "#Make X a 2D numpy array\n",
    "X = df[feature_cols].as_matrix()\n",
    "#Make y a 1D numpy array\n",
    "y = (df[class_cols]==0).astype(np.int).values.ravel()\n",
    "y_not_binary = (df[class_cols]).astype(np.int).values.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Taken from notebook 6\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[ 3.22011045]\n",
      " [ 1.2072735 ]\n",
      " [ 0.79098345]\n",
      " [ 0.50013852]\n",
      " [-1.70765195]\n",
      " [-0.60296561]\n",
      " [-1.72466313]]\n",
      "Accuracy of:  0.855324074074\n",
      "Wall time: 62.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "slr = StochasticLogisticRegression(0.1,1000, C=0.001) # take a lot more steps!!\n",
    "\n",
    "slr.fit(X,y)\n",
    "\n",
    "yhat = slr.predict(X)\n",
    "print(slr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[ 0.13428569]\n",
      " [ 0.01887707]\n",
      " [ 0.01679137]\n",
      " [-0.00573651]\n",
      " [-0.03575905]\n",
      " [-0.01603305]\n",
      " [-0.05844669]]\n",
      "Accuracy of:  0.891782407407\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class NewtonLogisticRegression(BinaryLogisticRegression):\n",
    "    def _hessian(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel()\n",
    "        return X.T @ np.diag(g*(1-g)) @ X - 2 * self.C \n",
    "    def fit(self,X,y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            inv_hessian = np.linalg.inv(self._hessian(Xb,y))\n",
    "            self.w_ +=  inv_hessian@gradient*self.eta # multiply by learning rate \n",
    "            \n",
    "nlr = NewtonLogisticRegression(0.1,1000, C=0.001) # take a lot more steps!!\n",
    "nlr.fit(X,y)\n",
    "\n",
    "yhat = nlr.predict(X)\n",
    "print(nlr)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.0001, optimize_func='steepest'):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        self.optimize_func = optimize_func\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = (y==yval).astype(np.int) # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            if self.optimize_func == 'stochastic':\n",
    "                 hblr = StochasticLogisticRegression(self.eta,self.iters,self.C)\n",
    "            elif self.optimize_func == 'steepest':\n",
    "                hblr = BinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            elif self.optimize_func == 'newton':\n",
    "                hblr = NewtonLogisticRegression(self.eta,self.iters,self.C)\n",
    "            hblr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ 0.08093106  0.18241802  0.17404447  0.08358793 -0.05967811  0.03953186\n",
      "  -0.05569466]\n",
      " [-0.10943317 -0.15751531 -0.16004672 -0.13135618 -0.02709511 -0.0853471\n",
      "  -0.01265203]\n",
      " [-0.17085057 -0.27019536 -0.27019536 -0.23320433 -0.23126378 -0.16183375\n",
      "  -0.14974222]\n",
      " [-0.17436727 -0.27103951 -0.25973252 -0.23233586 -0.23616838 -0.14860788\n",
      "  -0.13741636]]\n",
      "Accuracy of:  0.700231481481\n",
      "Wall time: 21.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(0.1,iterations=8,C=0.00001,optimize_func='steepest')\n",
    "lr.fit(X,y_not_binary)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y_not_binary,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ 0.13892291  0.55271487  0.37732628  0.27697194 -0.6653365   0.40599706\n",
      "  -0.53772524]\n",
      " [-0.3489043  -0.56701544 -0.03233117 -0.42628719 -0.06817394 -0.2420182\n",
      "   0.35024336]\n",
      " [-0.40540757 -0.48543675 -0.48981037 -0.53021689 -0.47277038 -0.48083865\n",
      "  -0.28122563]\n",
      " [-0.39097266 -0.58569383 -0.52518361 -0.33184822 -0.33685616 -0.14945411\n",
      "  -0.16006202]]\n",
      "Accuracy of:  0.708333333333\n",
      "Wall time: 11.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr_st = MultiClassLogisticRegression(0.1,iterations=50,C=0.00001,optimize_func='stochastic')\n",
    "lr_st.fit(X,y_not_binary)\n",
    "print(lr_st)\n",
    "\n",
    "yhat_st = lr_st.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y_not_binary,yhat_st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[  1.10386055e-03   1.55146904e-04   1.38003599e-04  -4.71440978e-05\n",
      "   -2.93885242e-04  -1.31789158e-04  -4.80548264e-04]\n",
      " [ -1.22545557e-03  -2.48577803e-05  -3.00007718e-05   2.82864652e-05\n",
      "    2.16281175e-04   6.26802083e-05   3.27865703e-04]\n",
      " [ -7.67913029e-04  -6.90017923e-05  -6.90017923e-05   3.85725889e-06\n",
      "    3.85724552e-05   4.82155481e-06   4.82155481e-05]\n",
      " [ -9.61969137e-04  -6.12873041e-05  -3.90010061e-05   1.50004085e-05\n",
      "    3.90316516e-05   6.42873976e-05   1.04467021e-04]]\n",
      "Accuracy of:  0.786458333333\n",
      "Wall time: 486 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(0.1,iterations=8,C=0.00001,optimize_func='newton')\n",
    "lr.fit(X,y_not_binary)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y_not_binary,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
