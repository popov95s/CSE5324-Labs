{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "The dataset we chose contains car evaluation data derived from a hierarchical decision model developed initially for a demonstration of a decision making model and can be found at [[1]](#footnote1). The authors The dataset contains 6 attributes related to either price or technical characteristics. The 7th attribute represents the estimated class of the car and is based on all other attributes. The dataset consists of 1728 entries and is stripped of structural attributes, which means all attributes are directly related to the estimated car class attribute. There are also three intermediate attributes – PRICE, TECH and COMFORT – which are related to the 6 main attributes. \n",
    "## Use case\n",
    "Choosing a vehicle to purchase can be a tedious process that involves hours of research, with studies showing American drivers spend an average of around 15 hours between realizing the need for a new car and making the purchase [[2]](#footnote2). 60% of this time is usually spent in online research of specifications and availability. Generally, although the most important attribute of a car is its ability to transport, the final decision is very often based on an amalgam of its price, safety and capacity. The main purpose for the collection of the dataset we chose was to “actively support the decision maker in the knowledge acquisition and evaluation stages of the decision making process” [[3]](#footnote3). \n",
    "## Prediction task\n",
    "The dataset uses a simple hierarchical model to classify cars in one of 4 categories: Unacceptable (unacc), Acceptable (acc), Good (good), Very Good (vgood). The criteria tree is displayed below. The goal of our prediction task is to correctly identify the class associated with the car based on the 6 attributes that are used in the evaluation model, without specifying the model structure itself. \n",
    "\n",
    "\n",
    "<img src='tree.png' label=\"Criteria tree\"/ height=500 width=500>\n",
    "This could be useful in many different scenarios, such as helping manufacturers determine whether or not a new car would be well accepted by the market. Even after years of research and development, some car manufacturers suffer big financial losses due to lack of proper competitor analysis in the market and target audience expectations [[4]](#footnote4). Thus, for our evaluation criteria on this dataset, we aim to maximize the number of correctly predicted car classes in the range unacceptable and very good, as described above. The higher the percentage, the more reliable and valuable our algorithm will be to said manufacturers upon releasing a new vehicle to the market. \n",
    "\n",
    "As a benchmark to compare our algorithm against, we consider how accurate car manufacturers are now about predicting how successful their newest model will be. Since the actual predictions a car company has for their models is not public information, we use a proxy to approximate the prediction accuracy: the percentage of cars on the market that are acceptable, good, or very good. We assume that the goal of a car company is to produce a car that is not unacceptable. Thus, the percentage of cars on the market that are unacceptable is similar to the prediction error that car manufacturers exhibit. In our data set, 70% of the cars are unacceptable. Thus, we estimate for the purposes of this exercise that car manufacturers have a 30% accuracy rate in predicting the acceptability of cars.\n",
    "\n",
    "\n",
    "The simplicity of the hierarchical decision model used in the training set could be a limitation to this performance and reliability, as we are only basing our results on the 6 attributes that are provided. A further drawback is the fact that all attributes are nominally assigned, which would fail to account for small differences of attribute values near the hard cutoff limits. \n",
    "\n",
    " \n",
    "\n",
    "### References\n",
    "&nbsp;<a name=\"footnote1\">1</a>: https://archive.ics.uci.edu/ml/datasets/Car+Evaluation <br>\n",
    "&nbsp;<a name=\"footnote2\">2</a>: https://www.elephant.com/blog/car-insurance/new-study-details-how-long-it-takes-before-car-shoppers-buy <br>\n",
    "&nbsp;<a name=\"footnote3\">3</a>: http://kt.ijs.si/MarkoBohanec/pub/Avignon88.pdf <br>\n",
    "&nbsp;<a name=\"footnote4\">4</a>: https://www.popularmechanics.com/cars/g1766/10-cars-that-deserved-to-fail/?slide=3 <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of cars that are unacceptable: 0.7002314814814815\n",
      "4\n",
      "1382\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "#import s # Business Understandingcipy\n",
    "import scipy\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn import linear_model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "df = pd.read_csv('car.data')\n",
    "buying_maint_map = {'vhigh':3,'high':2,'med':1,'low':0}\n",
    "df['buy_price'] = df['buy_price'].map(buying_maint_map).astype(np.int)\n",
    "df['maint_price'] = df['maint_price'].map(buying_maint_map).astype(np.int)\n",
    "doors_map = {'2':0,'3':1,'4':2,'5more':3}\n",
    "df['doors'] = df['doors'].map(doors_map).astype(np.int)\n",
    "persons_map = {'2':0,'3':1,'4':2,'more':3}\n",
    "df['persons'] = df['persons'].map(persons_map).astype(np.int)\n",
    "trunk_map = {'small':0,'med':1,'big':2}\n",
    "df['trunk_size'] = df['trunk_size'].map(trunk_map).astype(np.int)\n",
    "safety_map = {'low':0,'med':1,'high':2}\n",
    "df['safety'] = df['safety'].map(safety_map).astype(np.int)\n",
    "class_map = {'unacc':0,'acc':1,'good':2,'vgood':3}\n",
    "df['class'] = df['class'].map(class_map).astype(np.int)\n",
    "\n",
    "\n",
    "feature_cols = ['buy_price','maint_price','doors','persons','trunk_size','safety']\n",
    "class_cols = ['class']\n",
    "\n",
    "unacc_percent = len(df[df['class']==0])/len(df['class'])\n",
    "print('Percent of cars that are unacceptable:',unacc_percent)\n",
    "\n",
    "#Make X a 2D numpy array\n",
    "X = df[feature_cols].as_matrix()\n",
    "#Make y a 1D numpy array\n",
    "y = (df[class_cols]==0).astype(np.int).values.ravel()\n",
    "y_not_binary = (df[class_cols]).astype(np.int).values.ravel()\n",
    "print(max(y_not_binary)+1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_not_binary, test_size=0.2, shuffle=True)\n",
    "print(len(X_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only data processing/preparation we do is to convert the text categories such as 'vhigh', 'small', 'unacc', etc. into integer values. Since these classes are ordered, we use integers instead of one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class NLayerPerceptron(object):\n",
    "    def __init__(self, n_hidden=30, n_hidden_layers=2,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None, \n",
    "                 cost_function='quadratic', activation_method='sigmoid'):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.cost_function = cost_function\n",
    "        self.activation_method = activation_method\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        Ws = [None]*self.n_hidden_layers\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            #if i!=self.n_hidden_layers-1:\n",
    "            if i==0:\n",
    "                Wi_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "                Wi = np.random.uniform(-1.0, 1.0,size=Wi_num_elems)\n",
    "                Wi = Wi.reshape(self.n_hidden, self.n_features_ + 1)\n",
    "            elif i==self.n_hidden_layers-1:\n",
    "                Wi_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "                Wi = np.random.uniform(-1.0, 1.0,size=Wi_num_elems)\n",
    "                Wi = Wi.reshape(self.n_output_, self.n_hidden + 1)\n",
    "            else:\n",
    "                Wi_num_elems = (self.n_hidden + 1)*self.n_hidden\n",
    "                Wi = np.random.uniform(-1.0, 1.0,size=Wi_num_elems)\n",
    "                Wi = Wi.reshape(self.n_hidden, self.n_hidden + 1)\n",
    "            \n",
    "            Ws[i] = Wi\n",
    "        return Ws\n",
    "    \n",
    "    #used for relu\n",
    "    def _sigmoid(self,z):\n",
    "        return expit(z)\n",
    "    \n",
    "    def _activation(self,z):\n",
    "        if self.activation_method=='sigmoid':\n",
    "            return expit(z)\n",
    "        elif self.activation_method=='linear':\n",
    "            return z\n",
    "        elif self.activation_method=='relu':\n",
    "            return np.maximum(0,z.copy())\n",
    "        elif self.activation_method=='silu':\n",
    "            return z*expit(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, Ws):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        meansquaresum = 0\n",
    "        for Wi in Ws:\n",
    "            meansquaresum += np.mean(Wi[:, 1:]**2)\n",
    "        return (lambda_/2.0) * np.sqrt(meansquaresum)\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,Ws):\n",
    "        if self.cost_function=='quadratic':\n",
    "            return self._quad_cost(A3,Y_enc,Ws)\n",
    "        elif self.cost_function=='cross':\n",
    "            return self._cross_cost(A3,Y_enc,Ws)\n",
    "    \n",
    "    def _quad_cost(self,A3,Y_enc,Ws):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, Ws)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _cross_cost(self,A3,Y_enc,Ws):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
    "        L2_term = self._L2_reg(self.l2_C, Ws)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _feedforward(self, X, Ws):\n",
    "        \"\"\"Compute feedforward step\n",
    "        \"\"\"\n",
    "        As = [None]*(self.n_hidden_layers+1)\n",
    "        Zs = [None]*self.n_hidden_layers\n",
    "        for i in range(len(As)):\n",
    "            if i==0:\n",
    "                As[0] = self._add_bias_unit(X, how='column')\n",
    "                As[0] = As[0].T\n",
    "            else:\n",
    "                Zs[i-1] = Ws[i-1] @ As[i-1]\n",
    "                if i!=len(As)-1:\n",
    "                    As[i] = self._activation(Zs[i-1])\n",
    "                    As[i] = self._add_bias_unit(As[i], how='row')\n",
    "                else:\n",
    "                    As[i] = self._sigmoid(Zs[i-1])\n",
    "\n",
    "        return As, Zs\n",
    "    \n",
    "    def _get_gradient(self, As, Zs, Y_enc, Ws):\n",
    "        if self.cost_function=='quadratic':\n",
    "            return self._quad_gradient(As,Zs,Y_enc,Ws)\n",
    "        elif self.cost_function=='cross':\n",
    "            return self._cross_gradient(As,Zs,Y_enc,Ws)\n",
    "        \n",
    "    \n",
    "    def _quad_gradient(self, As, Zs, Y_enc, Ws):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        Vs = [None]*self.n_hidden_layers\n",
    "        grads = [None]*self.n_hidden_layers\n",
    "        for i in range(self.n_hidden_layers,0,-1):\n",
    "            if i==self.n_hidden_layers:\n",
    "                Vs[i-1] = -2*(Y_enc-As[i])*As[i]*(1-As[i])\n",
    "                grads[i-1] = Vs[i-1] @ As[i-1].T\n",
    "            elif self.n_hidden_layers > 2 and i <= self.n_hidden_layers - 2 and i > 0:\n",
    "                if self.activation_method == 'sigmoid':\n",
    "                    Vs[i-1] = As[i]*(1-As[i])*(Ws[i].T @ Vs[i][1:,:])\n",
    "                elif self.activation_method == 'linear':\n",
    "                    Vs[i-1] = (Ws[i].T @ Vs[i][1:,:])\n",
    "                elif self.activation_method == 'relu':\n",
    "                    Vs[i-1] = Ws[i].T @ Vs[i][1:,:]\n",
    "                    Vs[i-1][Zs[i-1]<=0] = 0 \n",
    "#                 elif self.activation_method == 'silu':\n",
    "#                     temp = self._sigmoid(Zs[i])*((1-As[i]))\n",
    "#                     temp2= (As[i] + temp)\n",
    "#                     Vs[i-1] = Ws[i].T @ Vs[i][1:,:] * temp2\n",
    "                    \n",
    "                grads[i-1] = Vs[i-1][1:,:] @ As[i-1].T\n",
    "            else:\n",
    "                if self.activation_method == 'sigmoid':\n",
    "                    Vs[i-1] = As[i]*(1-As[i])*(Ws[i].T @ Vs[i])\n",
    "                elif self.activation_method == 'linear':\n",
    "                    Vs[i-1] = (Ws[i].T @ Vs[i])\n",
    "                elif self.activation_method == 'relu':\n",
    "                    Vs[i-1] = Ws[i].T @ Vs[i]\n",
    "                    Vs[i-1][Zs[i-1]<=0] = 0 \n",
    "#                 elif self.activation_method == 'silu':\n",
    "#                     temp = self._sigmoid(Zs[i])*((1-As[i]))\n",
    "#                     temp2 = (As[i] + temp)\n",
    "#                     Vs[i-1] = Ws[i].T @ Vs[i] * temp2\n",
    "                grads[i-1] = Vs[i-1][1:,:] @ As[i-1].T\n",
    "        \n",
    "        return grads\n",
    "\n",
    "    \n",
    "    def _cross_gradient(self, As, Zs, Y_enc, Ws):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        \n",
    "        Vs = [None]*self.n_hidden_layers\n",
    "        grads = [None]*self.n_hidden_layers\n",
    "        for i in range(self.n_hidden_layers,0,-1):\n",
    "            if i==self.n_hidden_layers:\n",
    "                Vs[i-1] = As[i]-Y_enc\n",
    "                grads[i-1] = Vs[i-1] @ As[i-1].T\n",
    "            elif self.n_hidden_layers > 2 and i <= self.n_hidden_layers - 2 and i > 0:\n",
    "                if self.activation_method == 'sigmoid':\n",
    "                    Vs[i-1] = As[i]*(1-As[i])*(Ws[i].T @ Vs[i][1:,:])\n",
    "                elif self.activation_method == 'linear':\n",
    "                    Vs[i-1] = (Ws[i].T @ Vs[i][1:,:])\n",
    "                elif self.activation_method == 'relu':\n",
    "                    Vs[i-1] = Ws[i].T @ Vs[i][1:,:]\n",
    "                    Vs[i-1][Zs[i-1]<=0] = 0 \n",
    "#                 elif self.activation_method == 'silu':\n",
    "#                     Vs[i-1] = Ws[i].T @ Vs[i][1:,:] * (As[i] + (1-As[i])*self._sigmoid(Zs[i]))\n",
    "                grads[i-1] = Vs[i-1][1:,:] @ As[i-1].T\n",
    "            else:\n",
    "                if self.activation_method == 'sigmoid':\n",
    "                    Vs[i-1] = As[i]*(1-As[i])*(Ws[i].T @ Vs[i])\n",
    "                elif self.activation_method == 'linear':\n",
    "                    Vs[i-1] = (Ws[i].T @ Vs[i])\n",
    "                elif self.activation_method == 'relu':\n",
    "                    Vs[i-1] = Ws[i].T @ Vs[i]\n",
    "                    Vs[i-1][Zs[i-1]<=0] = 0 \n",
    "#                 elif self.activation_method == 'silu':\n",
    "#                     Vs[i-1] = Ws[i].T @ Vs[i] * (As[i] + (1-As[i])*self._sigmoid(Zs[i]))\n",
    "                grads[i-1] = Vs[i-1][1:,:] @ As[i-1].T\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        As,_ = self._feedforward(X, self.Ws)\n",
    "        y_pred = np.argmax(As[-1], axis=0)\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.Ws = self._initialize_weights()\n",
    "        #gradients holds the gradients at each layer and iteration, but does not correctly separate them for some reason\n",
    "        gradients = [[]]*self.n_hidden_layers\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            As, Zs = self._feedforward(X_data,self.Ws)\n",
    "            \n",
    "            cost = self._cost(As[-1],Y_enc,self.Ws)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            grads = self._get_gradient(As=As, Zs=Zs, Y_enc=Y_enc,\n",
    "                                              Ws=self.Ws)\n",
    "            \n",
    "            for j in range(len(self.Ws)):\n",
    "                self.Ws[j] -= self.eta * grads[j]\n",
    "                #append to gradients array (does not correctly split, so needs extra work )\n",
    "                gradients[j].append(np.mean(grads[j]))\n",
    "        self.gradients_=[]\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            self.gradients_.append(gradients[0][i:][::self.n_hidden_layers])\n",
    "                \n",
    "            \n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = dict(n_hidden=10, \n",
    "              n_hidden_layers=4,\n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              epochs=500, # iterations\n",
    "              eta=0.001,  # learning rate\n",
    "              random_state=1,\n",
    "              cost_function='quadratic',\n",
    "              activation_method='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 500/500"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.702312138728\n",
      "F1 acc:  0.630568963308\n",
      "Wall time: 1.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "nn = NLayerPerceptron(**params)\n",
    "nn.fit(X_train, y_train, print_progress=10)\n",
    "yhat = nn.predict(X_test)\n",
    "print('Test acc:',accuracy_score(y_test,yhat))\n",
    "sample_weights = [1.5,1.0,2.0,2.5]\n",
    "weights = [sample_weights[x] for x in y_test ]\n",
    "print('F1 acc: ', f1_score(y_test,yhat,average='weighted', sample_weight=weights) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tune the hyper-parameters of cost function, activation method and number of layers, we used our evaluation criteria of the F1 score with sample weights of [1.5,1.0,2.0,2.5] for each of the classes and the train_test_split that we found to do a better job than a Stratified KFold. The advantage of using a simple train_test_split is that it drives the number of models we need to build down by a factor of how many folds we decide to have in our KFold. The other parameters, such as neurons to train for each layer, C, number of iterations and learning rate were kept constant across this tuning. It is important to note that they could also have an effect on our performance, but due to the high number of models that needed to be trained already and the lack of high-end equipment, we decided to keep those constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: quadratic | Activation: sigmoid | Layers: 2 | F1 Score: 0.889938274615 | Accuracy: 0.921965317919\n",
      "Cost: quadratic | Activation: sigmoid | Layers: 3 | F1 Score: 0.955292578755 | Accuracy: 0.945086705202\n",
      "Cost: quadratic | Activation: sigmoid | Layers: 4 | F1 Score: 0.877827627293 | Accuracy: 0.904624277457\n",
      "Cost: quadratic | Activation: sigmoid | Layers: 5 | F1 Score: 0.90544254349 | Accuracy: 0.921965317919\n",
      "Cost: quadratic | Activation: sigmoid | Layers: 6 | F1 Score: 0.884559422785 | Accuracy: 0.913294797688\n",
      "Cost: quadratic | Activation: sigmoid | Layers: 7 | F1 Score: 0.833543386575 | Accuracy: 0.893063583815\n",
      "Cost: quadratic | Activation: linear | Layers: 2 | F1 Score: 0.64214046272 | Accuracy: 0.705202312139\n",
      "Cost: quadratic | Activation: linear | Layers: 3 | F1 Score: 0.630568963308 | Accuracy: 0.702312138728\n",
      "Cost: quadratic | Activation: linear | Layers: 4 | F1 Score: 0.630568963308 | Accuracy: 0.702312138728\n",
      "Cost: quadratic | Activation: linear | Layers: 5 | F1 Score: 0.630568963308 | Accuracy: 0.702312138728\n",
      "Cost: quadratic | Activation: linear | Layers: 6 | F1 Score: 0.630568963308 | Accuracy: 0.702312138728\n",
      "Cost: quadratic | Activation: linear | Layers: 7 | F1 Score: 0.630568963308 | Accuracy: 0.702312138728\n",
      "Cost: quadratic | Activation: relu | Layers: 2 | F1 Score: 0.785569569008 | Accuracy: 0.760115606936\n",
      "Cost: quadratic | Activation: relu | Layers: 3 | F1 Score: 0.630568963308 | Accuracy: 0.702312138728\n",
      "Cost: quadratic | Activation: relu | Layers: 4 | F1 Score: 0.536977751328 | Accuracy: 0.476878612717\n",
      "Cost: quadratic | Activation: relu | Layers: 5 | F1 Score: 0.630568963308 | Accuracy: 0.702312138728\n",
      "Cost: quadratic | Activation: relu | Layers: 6 | F1 Score: 0.630568963308 | Accuracy: 0.702312138728\n",
      "Cost: quadratic | Activation: relu | Layers: 7 | F1 Score: 0.00399985778283 | Accuracy: 0.0260115606936\n",
      "Cost: cross | Activation: sigmoid | Layers: 2 | F1 Score: 0.952554307042 | Accuracy: 0.947976878613\n",
      "Cost: cross | Activation: sigmoid | Layers: 3 | F1 Score: 0.964770093822 | Accuracy: 0.956647398844\n",
      "Cost: cross | Activation: sigmoid | Layers: 4 | F1 Score: 0.950373846258 | Accuracy: 0.947976878613\n",
      "Cost: cross | Activation: sigmoid | Layers: 5 | F1 Score: 0.952197959881 | Accuracy: 0.947976878613\n",
      "Cost: cross | Activation: sigmoid | Layers: 6 | F1 Score: 0.93300552162 | Accuracy: 0.939306358382\n",
      "Cost: cross | Activation: sigmoid | Layers: 7 | F1 Score: 0.914760715205 | Accuracy: 0.933526011561\n",
      "Cost: cross | Activation: linear | Layers: 2 | F1 Score: 0.630568963308 | Accuracy: 0.702312138728\n",
      "Cost: cross | Activation: linear | Layers: 3 | F1 Score: 0.630568963308 | Accuracy: 0.702312138728\n",
      "Cost: cross | Activation: linear | Layers: 4 | F1 Score: 0.630568963308 | Accuracy: 0.702312138728\n",
      "Cost: cross | Activation: linear | Layers: 5 | F1 Score: 0.630568963308 | Accuracy: 0.702312138728\n",
      "Cost: cross | Activation: linear | Layers: 6 | F1 Score: 0.630568963308 | Accuracy: 0.702312138728\n",
      "Cost: cross | Activation: linear | Layers: 7 | F1 Score: 0.630568963308 | Accuracy: 0.702312138728\n",
      "Cost: cross | Activation: relu | Layers: 2 | F1 Score: 0.653670159211 | Accuracy: 0.664739884393\n",
      "Cost: cross | Activation: relu | Layers: 3 | F1 Score: 0.630568963308 | Accuracy: 0.702312138728\n",
      "Cost: cross | Activation: relu | Layers: 4 | F1 Score: 0.630568963308 | Accuracy: 0.702312138728\n",
      "Cost: cross | Activation: relu | Layers: 5 | F1 Score: 0.630568963308 | Accuracy: 0.702312138728\n",
      "Cost: cross | Activation: relu | Layers: 6 | F1 Score: 0.630568963308 | Accuracy: 0.702312138728\n",
      "Cost: cross | Activation: relu | Layers: 7 | F1 Score: 0.630568963308 | Accuracy: 0.702312138728\n"
     ]
    }
   ],
   "source": [
    "cost_functions = ['quadratic', 'cross']\n",
    "activation_functions = ['sigmoid','linear', 'relu']\n",
    "num_hidden_layers= [x for x in range(2,8)]\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "results = []\n",
    "for cost in cost_functions:\n",
    "    for activation in activation_functions:\n",
    "        for num in num_hidden_layers:\n",
    "            params = dict(n_hidden=30, \n",
    "              n_hidden_layers=num,\n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              epochs=500, # iterations\n",
    "              eta=0.001,  # learning rate\n",
    "              random_state=1,\n",
    "              cost_function=cost,\n",
    "              activation_method=activation)\n",
    "            nn = NLayerPerceptron(**params)\n",
    "            nn.fit(X_train, y_train)\n",
    "            yhat = nn.predict(X_test)\n",
    "            f1score = f1_score(y_test,yhat,average='weighted', sample_weight=weights)\n",
    "            acc = accuracy_score(y_test,yhat)\n",
    "            print(\"Cost:\", cost, \"| Activation:\", activation, \"| Layers:\", num, \"| F1 Score:\", f1score, \"| Accuracy:\", acc )\n",
    "            results.append({'cost':cost, 'activation':activation, 'layers':num, 'f1_score':f1score, 'accuracy':acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14f81029b38>"
      ]
     },
     "execution_count": 805,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEFCAYAAADqujDUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14f83884e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "df = pd.DataFrame(results)\n",
    "df['f1_score']= df['f1_score'].astype(np.float64)\n",
    "df['cost_activation']=df['cost'] + \" \"+ df['activation'] \n",
    "sns.swarmplot(data=df, y='f1_score',x='layers', hue='cost_activation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the graph above that the sigmoid activation function performs significantly better than all other activation functions for our dataset. We can also notice that adding more hidden layers does not improve our performance. In general the cross entropy cost function yields better results for a sigmoid activation function ,while the same is not necessarily true for ReLu at lower number of layers. As the number of layers grows, the accuracy of ReLu or Linear activation does not seem to be affected. Since the functions are very similar for their values over 0, we can assume that the majority of values generated do not go under 0 for more than 3 layers, since the performance is nearly identical for both cost functions with those activation methods. The best model to use out of the 36 tested appears to be between Quadratic Sigmoid with 3 layers and Cross Sigmoid with 3 layers. Due to the fact that for all other layers the sigmoid activation performs much better with a cross entropy cost function, compared to the quadratic cost function, we can conclude it is better fit for our data. Thus, we can conclude that a Cross Sigmoid with 3 layers is the best combination of hyper-parameters to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient magnitude visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = dict(n_hidden=30, \n",
    "              n_hidden_layers=3,\n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              epochs=500, # iterations\n",
    "              eta=0.001,  # learning rate\n",
    "              random_state=1,\n",
    "              cost_function='cross',\n",
    "              activation_method='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 500/500"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.956647398844\n",
      "F1 acc:  0.964770093822\n",
      "Wall time: 1.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "nn = NLayerPerceptron(**params)\n",
    "nn.fit(X_train, y_train, print_progress=10)\n",
    "yhat = nn.predict(X_test)\n",
    "print('Test acc:',accuracy_score(y_test,yhat))\n",
    "sample_weights = [1.5,1.0,2.0,2.5]\n",
    "weights = [sample_weights[x] for x in y_test ]\n",
    "print('F1 acc: ', f1_score(y_test,yhat,average='weighted', sample_weight=weights) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEFCAYAAAABjYvXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14f80b03ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.subplot(1,1,1)\n",
    "for i in range(0,params['n_hidden_layers']):\n",
    "#     print(nn.gradients_[i])\n",
    "    plt.plot(np.abs(nn.gradients_[i]), label='w' + str(i) )\n",
    "plt.legend()\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows the average magnitutde of the gradients against the training iteration for our best performing model using 3 hidden layers, the cross entropy cost function and the sigmoid activation function. With these hyper-parameters, we can see that the gradients in the outer layer (w2) converge very fast to 0, and we can see that after the 100th iteration, the gradients in all 3 layers are very close to one another in terms of mean value. The gradients seem to stay fairly consistent and the plot does not contain any value spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
