{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "The dataset we chose contains car evaluation data derived from a hierarchical decision model developed initially for a demonstration of a decision making model and can be found at [[1]](#footnote1). The authors The dataset contains 6 attributes related to either price or technical characteristics. The 7th attribute represents the estimated class of the car and is based on all other attributes. The dataset consists of 1728 entries and is stripped of structural attributes, which means all attributes are directly related to the estimated car class attribute. There are also three intermediate attributes – PRICE, TECH and COMFORT – which are related to the 6 main attributes. \n",
    "## Use case\n",
    "Choosing a vehicle to purchase can be a tedious process that involves hours of research, with studies showing American drivers spend an average of around 15 hours between realizing the need for a new car and making the purchase [[2]](#footnote2). 60% of this time is usually spent in online research of specifications and availability. Generally, although the most important attribute of a car is its ability to transport, the final decision is very often based on an amalgam of its price, safety and capacity. The main purpose for the collection of the dataset we chose was to “actively support the decision maker in the knowledge acquisition and evaluation stages of the decision making process” [[3]](#footnote3). \n",
    "\n",
    "Furthermore, manufacturers have large vested interests in desinging cars that are acceptable to consumers. Developing a car that the market reacts to well can be the difference between a thriving company and a failing company. This is because bringing a car to market has a high level of fixed costs. These costs include: research, design, prototyping, sourcing, manufacuring, initial marketing, transporation, and storage. All these are costs that are incurred before any dollar of revenue from that product line comes in. This money is usually raised through issuing bonds or raising equity. A failed car can make a company default on its loans (because revenues do not cover costs) and for those issuing equity it can tank their equity value given that a share price represents the future cash flows of a company. A company with no profits has no positive cash flows, and thus the company value is destroyed. Thus, it is very important for manufacturers to have an estimate on whether or not their car will perform well. \n",
    "\n",
    "## Prediction task\n",
    "The dataset uses a simple hierarchical model to classify cars in one of 4 categories: Unacceptable (unacc), Acceptable (acc), Good (good), Very Good (vgood). The criteria tree is displayed below. The goal of our prediction task is to correctly identify the class associated with the car based on the 6 attributes that are used in the evaluation model, without specifying the model structure itself. \n",
    "\n",
    "\n",
    "<img src='tree.png' label=\"Criteria tree\"/ height=500 width=500>\n",
    "This could be useful in many different scenarios, such as helping manufacturers determine whether or not a new car would be well accepted by the market. Even after years of research and development, some car manufacturers suffer big financial losses due to lack of proper competitor analysis in the market and target audience expectations [[4]](#footnote4). Thus, for our evaluation criteria on this dataset, we aim to maximize the number of correctly predicted car classes in the range unacceptable and very good, as described above. The higher the percentage, the more reliable and valuable our algorithm will be to said manufacturers upon releasing a new vehicle to the market. \n",
    "\n",
    "As a benchmark to compare our algorithm against, we consider how accurate car manufacturers are now about predicting how successful their newest model will be. Since the actual predictions a car company has for their models is not public information, we use a proxy to approximate the prediction accuracy: the percentage of cars on the market that are acceptable, good, or very good. We assume that the goal of a car company is to produce a car that is not unacceptable. Thus, the percentage of cars on the market that are unacceptable is similar to the prediction error that car manufacturers exhibit. In our data set, 70% of the cars are unacceptable. Thus, we estimate for the purposes of this exercise that car manufacturers have a 30% accuracy rate in predicting the acceptability of cars.\n",
    "\n",
    "\n",
    "The simplicity of the hierarchical decision model used in the training set could be a limitation to this performance and reliability, as we are only basing our results on the 6 attributes that are provided. A further drawback is the fact that all attributes are nominally assigned, which would fail to account for small differences of attribute values near the hard cutoff limits. \n",
    "\n",
    " \n",
    "\n",
    "### References\n",
    "&nbsp;<a name=\"footnote1\">1</a>: https://archive.ics.uci.edu/ml/datasets/Car+Evaluation <br>\n",
    "&nbsp;<a name=\"footnote2\">2</a>: https://www.elephant.com/blog/car-insurance/new-study-details-how-long-it-takes-before-car-shoppers-buy <br>\n",
    "&nbsp;<a name=\"footnote3\">3</a>: http://kt.ijs.si/MarkoBohanec/pub/Avignon88.pdf <br>\n",
    "&nbsp;<a name=\"footnote4\">4</a>: https://www.popularmechanics.com/cars/g1766/10-cars-that-deserved-to-fail/?slide=3 <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of cars that are unacceptable: 0.7002314814814815\n",
      "4\n",
      "1382\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "#import s # Business Understandingcipy\n",
    "import scipy\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "df = pd.read_csv('car.data')\n",
    "buying_maint_map = {'vhigh':3,'high':2,'med':1,'low':0}\n",
    "df['buy_price'] = df['buy_price'].map(buying_maint_map).astype(np.int)\n",
    "df['maint_price'] = df['maint_price'].map(buying_maint_map).astype(np.int)\n",
    "doors_map = {'2':0,'3':1,'4':2,'5more':3}\n",
    "df['doors'] = df['doors'].map(doors_map).astype(np.int)\n",
    "persons_map = {'2':0,'3':1,'4':2,'more':3}\n",
    "df['persons'] = df['persons'].map(persons_map).astype(np.int)\n",
    "trunk_map = {'small':0,'med':1,'big':2}\n",
    "df['trunk_size'] = df['trunk_size'].map(trunk_map).astype(np.int)\n",
    "safety_map = {'low':0,'med':1,'high':2}\n",
    "df['safety'] = df['safety'].map(safety_map).astype(np.int)\n",
    "class_map = {'unacc':0,'acc':1,'good':2,'vgood':3}\n",
    "df['class'] = df['class'].map(class_map).astype(np.int)\n",
    "\n",
    "\n",
    "feature_cols = ['buy_price','maint_price','doors','persons','trunk_size','safety']\n",
    "class_cols = ['class']\n",
    "\n",
    "unacc_percent = len(df[df['class']==0])/len(df['class'])\n",
    "print('Percent of cars that are unacceptable:',unacc_percent)\n",
    "\n",
    "#Make X a 2D numpy array\n",
    "X = df[feature_cols].as_matrix()\n",
    "#Make y a 1D numpy array\n",
    "y = (df[class_cols]==0).astype(np.int).values.ravel()\n",
    "y_not_binary = (df[class_cols]).astype(np.int).values.ravel()\n",
    "print(max(y_not_binary)+1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_not_binary, test_size=0.2, shuffle=True)\n",
    "print(len(X_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only data processing/preparation we do is to convert the text categories such as 'vhigh', 'small', 'unacc', etc. into integer values. Since these classes are ordered, we use integers instead of one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWUAAABLCAYAAACoVJZJAAAYqUlEQVR4Ae2dWYwVxffHGzXqizBq1AcVRk2M0bC4Jb7IoKIx/8QBl/z0QcAt0QdgUGJ8kGERTUQYcHtjc0tMdMA1Jr8Io6IxPyODC2pMmBlcHtTMDKg8mOjA/edz/r/v/RfNXfrOXabv9DnJvd1dVV116lvVp0+dOlU9IZfL5SInR8ARcAQcgVQgcFwquHAmHAFHwBFwBAwBF8reERwBR8ARSBECLpRT1BjOiiPgCDgCLpS9DzgCjoAjkCIEXCinqDGcFUfAEXAEXCh7H3AEHAFHIEUIuFBOUWM4K46AI+AIuFD2PuAIOAKOQIoQcKGcosZwVhwBR8ARcKHsfcARcAQcgRQhcEKKeHFWHAFHwBFIBQL79++P+EHXXHNNNGHChGjPnj3R77//HrW0tESXXnqphdWDWdeU64FqA/I8cuRIdPjw4ai3t9eOvoVJA0Bv4iLoHytXrjQBQ98pR9u3b7e0CKIs0oYNG6J77rknuu666yKwA7OHHnoouuWWW6IHH3ywrpC4UK4rvPXJnA6yZMmS6NZbb7WOcvrpp9sDl+Rhqw9HnmvaEUCwfPjhh9EXX3xhQqYcvz/88IOl//LLL8slHZfxzz77bLRu3bp83Y477jjDY8aMGfmwep24+aJeyNYx31WrVkUTJ06M3nzzTStl/fr10dKlS6MpU6bY272ORXvWTYoAQqWnp8eG38cff3zZWqAN8uLnvqzSqaeeOiZVzy7iYwJ39YWi8Xz00UfRE088YaYLcuThgV588cVEWlD1XHgOo0GAtsPkpBHNyMjIUdfKkzT8REofXvf390fDw8P5vMI47sUeGpaleOyhIZH3wMCA5XXw4EET2sQTDr9xIpx8VXaYhnB+ItIWy0dpkhzJk3Li+OleypAZT2E6co/i47xwfeDAAas/53HCjjwW5EJ5LFCvokw6yvz586M5c+ZEF1xwgeWkzvPjjz9WkbPfWm8Ebr755uiEE06I0FSXL19u5idslJdddln+Bbtz505LQ7q1a9dGCxYsiDBPoeVCCB/snAyvMV9xjmCBEECMnugX2ETJ+9prr7X4zs5OK5eyJYCwF5PmhRdeiBh9nXfeeTaZRVmUzw8+RQhi0jMqo3wmwD744AMrN+R78eLFxjc2WMrfunWrpVE+lRxnzZplfHCEF+pGnakDv66uLisDhYS4LVu25MuC37lz51o8uIAzvEC81Ej/2GOPGVbEca+wIQ14xqlQWDxN1ddscu/U3Ajs27eP3pObN29ec1dknHN/5MiR3KZNm6ytOHINLV++3MJoR2hwcNCup0yZkuvr68tNmjQpt23bttzIyEiOsM7OTkvH/dOmTcu3e09Pj923Y8cOi+/q6rLr3t5eK6u7u9uuDx8+bNft7e2WryXO5XILFy7M7dy50y4piz6lsrieOXNmjjxF8Atv8AiRL/fAI3WAKLOlpSU3PDxs15X+UUd4IN/NmzcbVtOnT8/nTflDQ0N2De+k6+/vN6zmzJljPJMHP9ISBgkbYUV7xPkkjvyolwgM+JFfvYi3gVOTI0BHo6OqczZ5dcY1+4UedAQWD/+iRYvsYY8LRAkFCfT333/fBAXhy5YtM2EioRkXGAhkUbxs+k1ra6sJu927d5twlfD8559/jCcJZQl0CWDl2dbWlkO4wws8IPioh0iCUsJe4ToeOnQoN3Xq1FxHR4eCjjlKKFMGwpByOKfPU19dcwRHBC715pyXmWhgYOColwMvMfKhrq+//rqlB1tRHC/CqW8cY6Wv1dEn+qoea4xtBps3bzb7IcNHhrlOzYcAE0pM0n711VfGvMxRqokm22Seeuutt2xeQfELFy60oTb3T58+/Sj/WYbloni+mANw+7r33nstCZ4F9CNIZepe8QafIg3l8d2FyJ884nZrpS90pE579+6NTjrppELRx4RRBiYY7MzwBD+4+okw08ADXiZQyAvmGRFmCuzumFgmT54cTZo0yaLi9Vb6Rh5dKDcS7RqXxQOEbQ8b4GmnnWYdEad2p+ZCAOGGcEKgQhJ28VpIIGKbxlYbEkKmtbXVgrg/LoDDtOE5Nmq5v2Erxsb69NNPh0nsXMLtp59+is4//3wLoxzxSnmck9fMmTOPuV/p4hGXXHJJ9O2335p9Nx6n60L3Uh548IPneH31clEe8SPeJdiheXZ4cZH+5ZdftmRgOZbC2Sf64q3VJNc8TG+88YZNtKAh03FXrFiRf0iapBqZY1PCQ5NzAID/MMTkLfFKExcMCGM0OrlCcg8ChIk1qL29Pdq1a1d+4o8whA4LQaBQuHGOYELLZcILbZl88MAgLkzLvfBG2eFiEu6lPPENv9LmrcCEfxdffHFJTbkQHoQx4U358CyivjwXaMvwi+ANqaOjw7RslBkmD8ORBOnQwHnhgetY0fErQ91/rLjwcitCgAfjtttui6666qros88+s4f67bffts55xx135B/qijL1xA1BgCHzSy+9ZO02bdq06Pvvv7ch9A033GDDcOIRGLg9Ini4bmtrs/OTTz45Ouuss6KnnnrKhNhff/1lQueiiy6yZb8IGQT8xo0bowsvvNDupawHHngg+vTTT00TlBmCITtmENJeeeWV0a+//mrnmEIoVzwgaM8555zo8ssvt7IfffRR005/+eWXaM2aNabdP/nkk1bWM888Y3xzD8IN74dXXnnFzAyEnXLKKXktOwnYvBgwz7377rumgcMXwhIzBOdo5KrvmWeeGX333XfRa6+9Ft1///1mygMDsDrxxBPtRxyaNYKYlyLPDHUnz+eff97KOPvss401eKdc8IIP8GIEwUvgjz/+sPvVLknqUkmaCRinK7mhWFqySZIVjeNUHQJoJnSoOC1btixavXp1PNivEyAQ9t969lGGybNnz44GBwdNOKPl4VqGQKVcrrXqDsGD8MANTTwhQPArpv3RVBEMuIiJEIYIW1biYXK46667zLTFPQh4iLpigiANJhNc4iDc79Ca4YEXv8pEkBFO2eSBJopgwt0McxnpCKcMeIbQUvlh7hBhXpHpQ2GljvCJyx3EOXnH84AnRg6qr1wIdQ98EQ+/YCWzD/dRb8wt2PTvvvtu45+XIRq4hLH4Q/PmBUBaeIFoN2GkdLU41kwoM5zGaE5lACC0KwEmlYXoNPWoSC3AaJY86FDFyLEthkzhcLBEyDz33HOWgIePCTBs9PUgCWUWjjBh5eQIxBGomVAmYwQub1TekLzt9dYkbvfu3dH1119vb34XHPFm8OuxQoAFA2hZLLZAa2JBBnbRvr6+mnuz8EygnGDLRTgzDA89AsYKAy83XQjU1JZAZ+bHMAHBi1DWD5tU3F0nXVA4N1lDAC0Z2+nPP/9s/RSFAi0ZswD20VIjktFghUaOOQK3LYbUTEo5OQJxBGrqEqdZZOxjIjwCNJcotxrF+dERGGsEEMBaKgwv2OshTbTVkj9ct5wcgXII1EwoY/yW3RhNGWIGk6EhhOaMdhCaNCyixJ8M6iWSHBVVSd5H3egXTYdALfoGfVIuXrLvatEBk1ven5quW4wLhmsmlLEnSyhjl6NDI5AXLVqU79yVdnKGewwjkxAvAtdEkiA1PtJgA8bLIAnhf0ufLETxCT1co5gT0eiu0D0e5gjUE4GaTfThDoNNDj9HHhg8MOSQLjcUtBIEN3a80H2mWAXRhgppRIQVEvClJhAR8E7NhQCLJYpRJfZe+kqh/hLPe8eOHeb/zSQccyCFCL/i22+/Pbrzzjujhx9+uFAS075Hs4iiYGYe2HQIlOq3SSpTM01Z9mR899jyT6vMQvsyDGFXZpY7CSV9mJLk9d5775lvaJK0niYdCJTq3KVewKPhHrPFfffdZ14RCORiL/59+/ZFX3/9tS0iKCaUKT++kmw0PPk9zYcALpWl+m2SGtVEU6YD46TO6qGhoaG8K1Ghjs2Ms/ZDLae9kFbCvlxlsAFqY5VyaT2++RFg6bDmK8rVBtNWuMAinh6PCPovK7ZY3IAWznJcRnSF+ujnn39uK8Hi+fi1I1ALBGqiKSN8MUuwbDS00RXq0GK6kMBWnI6VbK6jzVh0rx/HNwL0jaTePKX6kVbMMfchl05WgZUapbE018kRqBcCVQtlnO8ZztGhUd2ZIEEzkR25GOOlBLbuYUljuXyU1o/ZQqBWCy9YwoxCoUlqocgXLZwcgeoQGIxeu+eq6N//859o821nJM6qaqGMhorp4KabbrJCx/sEB/bwrLpLMbqhfcfLqIT6sB1AuCWAnhz2dXByBEIEMM2igDLy0h4YbC9x6NAh27vkWEXzjOhfW/qjf4WZJDmv1W75SfPRlwjq+TmVpLxUko5P0SxevNi+rNBsvFdSz0Jp+bwOnyziSw8cndKPAF/U4MsitFeS/rply5bc3Llzj/oyR/pr2TgO+eoLn7libyR9RUXygDDwrhUxy9wwomKzZs2yis2fP9++pdWwwqssiI4N8DRAkk5eZXGpu51P7fAZHBfKqWuaggzRV/k0E4IkicBAgNO3+SxUo4mX/sGDBxtdbMXl6fNQEsrIAT7HVWuhXLX5Iok2rjR8eoafJvmOVfeVMn1HeIXvrFKtXdCyimOj6q3VikyGJmk7vvLM2oFi/tn15JuycaVttvkjZEI9ZFhDhbIqoGM9G7qeeeulUs8y0pp3ll9Mo2kT3Ov08LLqVX0/LiiVTmUoHddaKKN+F95LmH4qR/cW2oEOHkSkU17YSAt5qVC22jxMrzyIVx7iU9dKU48jZame5B+ehzzDSxhHXcQn4fqRh8LFb7weYT5KU49jTXeJqweDacyzUY3jdU8jAsl44gHH15k9NRB27AzHogK+eoHHB654EN5LCE/Sbdq0ydLgK614JpLQIJcsWWIrZtn2U8KD4/r16y1P4slb8Swtxz2VxVsSqvh2s9x81apVtqm7Nq7n6x7iIdy5Ds0Z/23yZIQL/+Ir5Jt8SQef8MCKyHpRWC58g62+JEKZ8M81e7tTP7zBQrxYbcxoQHgRL0GNts5m97QbeVCW7q1XfQrmW7FhJcM3NNKmjL1q3bp1ieyBjWgS+HGbcmVI01+YDMLmiD0SIqy9vT3X2tqaw07PdXd3t6VhnkV2yz179ticS0tLS27btm127/DwsKVj0pn26OrqMrvx0NCQxcsuTBw/yqbNOKccbMxKSxgTt4RzThnwKXvp4OBgjrJ1TQEbN27MzZgxI893X1+f3UMZypcy29raLF9jKsEfdtmwnFK3xPES3wMDA3m8lFc5vOATTKDe3l6rS2dnp12TR9huBCpM+RNG+lrblF1TLviqKh0ozaN0qupjk264U31JleXQqPpXxlX6UjP8ZXMjCC0MIoxPFuFayVJsrjEdQISz8hATA3uPsxkXW4vyQVQ0NuzDnNMvSMMGXGw1ypYGECsS0aw1JFfZxGl0x6pXtipF40XDLZSW9KzOpexwmwS0YVZRwhd8yzwS8kCZcZ9vY+6/f9Qj/pOmWig8vJfzOF6MOsACN1VGAOSFi6PwYkEb/BAOXnzqSXjxOSiNDKTh68OqV199tRUd1oUyGkENtSk3okL1KoOOTCOyMIZhHMMfPWj1KjMt+bLCjQceYtk7wzqGs3rQ08Jns/AhQVdsDxgED0JFy8gRNiKEDH7i3MumX6GgILyYDzlthamDYb1e9pg4GM5LMKsMylaa8AWs9uaTb6QR8QJJSjxD9KeQPvnkE/N/DwUg8eBUanm88hBeYMKLK8SLlxgvD/Y2Aa9wFWghvHhp8TKaOnWqss8fi7VXPkGNTlwoJwQSAZwVIRyHhE6ddAvV+L1+fSwC8Yc7FHxhaglcbLrh9gWkCSfswnuKnVMG9lf2OOfFitBlbw+0ykLbmkrLliAO84UvwkPBHMZzXqxOvMzjhL2dZyup90WxvOGZxU1oxHG+qXcpkr0dOzUjFspgG1eIc/Ir9sIrle9o4tx8MRrU/B5HoEIEQkGiTbb0lRNlhcYn4hxzAaT0nCOM0XiJR6DyVetQOOJe1t/fr2zyR9IwwkO4oH0ikClfK3BD/sgbwQTxZWuR0jJajAs9pWnksRhe+lABvCCM+foRJiI0+l27dh2FFyMFJjTBg3gm+ph0RVsW8cIAP9Vf4fU6ZlJTLrXlYr2AJt8bb7yx6HCMB43hlYiHhG/HMTscfwAwncgOqfQc+Zy7vsochjfT+dq1a5uJ3cS8ou3SlpgkVq9ebd4YaK4I0I8//tjyYfg+ceJE02hpc4QBGyWhXaIFIjTQAhGYxGNGQsAiSPi2IEIDDZh4bMt79+61PoEWKAHM0B67Kdo6vLDHBzzIdMCRPWyw1c6bN892XiRvCD6k1SLIEHAQ/MMfPKku8IH5Ia7h2w1V/MHrO++8YznE8eKlg80YTLq7u6PJkyfb8wAeCHC04DhemGJ4nnjB0S7sqY19+fHHHzfTEBhi8gjrS7nkjeAmHmK/dtqrFvWtydadVWA8JreuWbNmTMq94oorigplTTiIMYQy9j+EdVwo0/jxMO5DKL/66qvKoimPjzzySFPyXYxphBgPOF9zR1DRrrSf2pB2DzVhTAOhvR4NjfkMBCD3oj3jMidCE2TCEAGBYET4S2CHmh2CFW0QgYtQIV/ywm0szgNCTO5y8Eb5EEIWvhFwhPGjLPLSpF9omlE+4rXYUYI+ifkizms5vOI8FMKL+oAtLy+wQbCy9wn14hN25AGWYEF9SctogboSLgx4OYZtU6y+5cIzKZTLgZKGeDU8HSEcpqWBN+chOQISypgd0tSO9C+ESRoIjZ9RwFisJkxD/eM8ZNJ8EQfBrx2BeiCAIP7zzz8ta7QuhGBaBGFa+ACcQpN/9WiPZsnz/2cWmoXjlPPJsAebHZoINkSGe/picspZL8seEyjY7YpRufhi943HcNofWyV2W4a6s2fPNres8VhXr1NtEXBNubZ42oPIbC/EkcmccAKvxsU1PDvslqWoXHype8dTHJooNlxRmjRT8eTHdCLgQrlAu6Dl8IvbAAmD9IAVSqc0Shc6qxcoqmQQmpbKKpmwSKT444ibT0gMpyHVMX5NHGHcCw+k4wWD879IdRWP8XjlwZE0SlfsfoWPl2O8vuOlXl6P+iKQWfMFJgVMC7gEMQvNZAPEDKw2YJHZgSWpmCQwRzDDijDC5QihSRgasYSaBFW1zcYDjVvSaB5sCXOOuFAxgYJpgR8O8LhHYccjf/imbqTDNIG3B0TdiWfRCLPRuARhmoEf6kg8OIEXs/Lko3KJ5xpccMHCO0AYkYYXFUfypkzydnIEHIH/ImC7b2T0b8WKFbYJOJu/cM5GI2zSAhHGBuFs1sLGJdqEhPj9+/fntm7dmuvo6LC0CxYsyPHlBqinpye/CQzX3EtYo4mFSNokZvv27bYBDpu5qI4HDhywOupam9Zw399//211Z5MWaMOGDbYJOfcTT1o2u1+5cqV9qYJNdIgL49kohg1yRGDFRjoQmPDBA4h8fON8g8L/HAFDINPmCzQ6tEaG3fzQ5tDi5CyPDyRaHL6LaNWQ9iPAdxFtkLT4K5I2LSRtXQtM0GThUeHYfYnjB//4uFJ3iGu0Yfwv8WmF0Goh6itCu0aTZqRA/jjch/ZknPJxcxIRx+ozzbTLr1XxfnQEHIH/QyCz5guqj/AJbb4ILX4MufnJWZy0EmjqOAzdEVwIJVYDQaw2Go25QXnW6ihedcRsgVCEN2zDIY8IXq6pLyYMTDmkIX24ECA8h08WQrB4AGd80vKyCvMtdH8opGtVV8/HERhvCGRaKNOYeEZIeKERImjR8hAwCF7ZYLVLmna5YqUP2jUCTMtN2e6QvPBN5cgvXEPfqM4DTxC2XGy/S5cuNds5deVFwxGbOISdGJctBDIjAm08hG0ZTZgw6k5dqDPEUlR2+kKQI6wRwIwUWGqqeDRpyiGM/RjAgTDKBWOW4zIK4evA/NyubND5nyMQZXZFH0JFngRaqkl/QDgQzuQWpg2G2QgkNi5BsGhfALRPBA7pGL4jsNgKUW5Q5557rnUv9q+AMA+EmqQF1vGPsn777bfom2++sRcL9YB/+IYYIWBygKifltKGS13BAqFLnTF1INAhhDDX3AeO5M0vjOeFRbzCwIiXBS8J+IA/cJcGzsuvFktUjUH/cwSaGIHMCuUmbrOyrCMMEYAjIyPHuMKVvdkTOAKOwJgi4EJ5TOGvfeEIZMwFmAjQRhutode+Rp6jI5AtBFwoZ6u9vbaOgCOQcgQyP9GX8vZx9hwBRyBjCLhQzliDe3UdAUcg3Qi4UE53+zh3joAjkDEEXChnrMG9uo6AI5BuBFwop7t9nDtHwBHIGAIulDPW4F5dR8ARSDcCLpTT3T7OnSPgCGQMARfKGWtwr64j4AikGwEXyuluH+fOEXAEMoaAC+WMNbhX1xFwBNKNgAvldLePc+cIOAIZQ8CFcsYa3KvrCDgC6UbAhXK628e5cwQcgYwh8L9mUKzFdr04awAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Case\n",
    "\n",
    "To manufacturers, our model's effectiveness depends on how it identifies cars that will get a good market reaction. This will allow the manufacturers to sell cars that will generate profits above their fixed costs and stop them from making cars that will lose them money. \n",
    "\n",
    "Manufacturer profits are estimated by: (simplified equation) \n",
    "\n",
    "<b>\n",
    "Profits = (avg profit from good car lines)(# of succeful car lines) - (avg loss from unacceptable cars)(# of unacceptable cars)\n",
    "</b>\n",
    "\n",
    "What this shows is that not only do we need to be sure that all the cars the manufacturers are making only good cars, we have to help them make as many of them as possible. This means that not only does our model needs to be right when is says a car is a good opportunity, but also needs to be able to identify as many opportunities as possible. \n",
    "\n",
    "What we have mentioned above is that our model needs to be good at both <b><i> recall </i></b> and <b><i> precision </i></b>. These can be sometimes competing efforts. To acheive the best balance of the two there exists a famous metric that takes the harmonic average of recall and precision to offer a point that gives the best balance of the two: F1 Score.\n",
    "\n",
    "### F1 Score: \n",
    "\n",
    "The F1 score is the harmonic average between a test's <b><i> recall </i></b> and <b><i> precision </i></b> scores. F1 is best at 1, and worst at 0. \n",
    "\n",
    "The F1 score calculation is: \n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "#### Recall\n",
    "Good recall is the abillity to mark as many class C instances as class C. This means that when a model with perfect recall is given a class, the model will be able to identify every actual member of class C as class C. Note, this does not mean that all instances marked as Class C are indeed class C, just that those that are class C are marked class C. Thus, this metric is concerned with reducing false negatives. \n",
    "\n",
    "In our case, recall is the ability for our model to be able to identity all good cars as good or all unacceptable cars as unacceptable (and so on and so forth). A high recall on Very good for example would meean that any instance that is very good will be marked as good. It does not guaruntee that all cars marked as very good are indeed very good. \n",
    "\n",
    "\n",
    "#### Precision\n",
    "Precision is the ability for a model to be correct when it denotes a class as a certain class. This means that a model with high precision on a certain class will be accurate when it says a test case is of that class. A model with perfect precision on that class will allow the user to know that if that class was identified as class c, it is class c. There is however no guartuntee that the model will catch all the class c's in the test set - that is what recall is for. Thus, this metric is concerned with reducing false positives. \n",
    "\n",
    "In our case, precision is the abillity for the model to be correct when it labels a car as very good for example. This metric is not concerned with whether or not we capture all the good cars, but with our ability to only cars very good if they are indeed very good.\n",
    "\n",
    "### Aditional Customization: \n",
    "\n",
    "The F1 score is calculated for every class, and then our version takes a macro average accross the classes. For our business case however, we notice that there are classes that are more important than others to improve recall and precision on. These classes are: Unacceptable (unacc), Very Good (vgood). The reason is beacuse firstly, consumers are most interested in the best of cars, and the most losses come to a company from making totally unacceptable cars. And judging by our dataset, where most cars are unacceptable, it is important to make sure that manufacturers avoid creating these cars. \n",
    "\n",
    "Furthermore, it is more important that the cars they do end up making are in fact very good, as opposed to seeing a good car as unacceptable. The reason being once again associated with the fixed costs and time associated with manufacturing a bad car vs. missing a good car. One way to create good weigths for this would be to see what is worse, missing a good car or creating a bad car. Generally, creating a bad car is worse because of the reputaion costs and fixed costs associated with it. \n",
    "\n",
    "A good ratio may be to weight F1 by class based upon how manufacturer profits respond to missed opportunities or wasted resources. If we had the data to determine the exact ratio of these economic losses it would help us explain a better F1 ratio. However, given most of this is private information we will estimate this ratio.\n",
    "\n",
    "\n",
    "Thus, we applied weights to the F1 scores by class. These weigths are:</br>\n",
    "<br>\n",
    "<br>\n",
    "Class: unacc, acc, good, vgood\n",
    "<br>\n",
    "Weigths: [1.5,1.0,2.0,2.5]\n",
    "\n",
    "### Final Evaluation Metric:\n",
    "\n",
    "Thus, our final evaluation metric is an F1 score with classes: [unacc, acc, good, vgood] weighted by scaling factors of [1.5,1.0,2.0,2.5].\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class NLayerPerceptron(object):\n",
    "    def __init__(self, n_hidden=30, n_hidden_layers=2,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None, \n",
    "                 cost_function='quadratic', activation_method='sigmoid'):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.cost_function = cost_function\n",
    "        self.activation_method = activation_method\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        Ws = [None]*self.n_hidden_layers\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            #if i!=self.n_hidden_layers-1:\n",
    "            if i==0:\n",
    "                Wi_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "                Wi = np.random.uniform(-1.0, 1.0,size=Wi_num_elems)\n",
    "                Wi = Wi.reshape(self.n_hidden, self.n_features_ + 1)\n",
    "            elif i==self.n_hidden_layers-1:\n",
    "                Wi_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "                Wi = np.random.uniform(-1.0, 1.0,size=Wi_num_elems)\n",
    "                Wi = Wi.reshape(self.n_output_, self.n_hidden + 1)\n",
    "            else:\n",
    "                Wi_num_elems = (self.n_hidden + 1)*self.n_hidden\n",
    "                Wi = np.random.uniform(-1.0, 1.0,size=Wi_num_elems)\n",
    "                Wi = Wi.reshape(self.n_hidden, self.n_hidden + 1)\n",
    "            \n",
    "            Ws[i] = Wi\n",
    "        return Ws\n",
    "    \n",
    "\n",
    "    \n",
    "    def _activation(self,z):\n",
    "        if self.activation_method=='sigmoid':\n",
    "            return expit(z)\n",
    "        elif self.activation_method=='linear':\n",
    "            return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, Ws):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        meansquaresum = 0\n",
    "        for Wi in Ws:\n",
    "            meansquaresum += np.mean(Wi[:, 1:]**2)\n",
    "        return (lambda_/2.0) * np.sqrt(meansquaresum)\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,Ws):\n",
    "        if self.cost_function=='quadratic':\n",
    "            return self._quad_cost(A3,Y_enc,Ws)\n",
    "        elif self.cost_function=='cross':\n",
    "            return self._cross_cost(A3,Y_enc,Ws)\n",
    "    \n",
    "    def _quad_cost(self,A3,Y_enc,Ws):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, Ws)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _cross_cost(self,A3,Y_enc,Ws):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
    "        L2_term = self._L2_reg(self.l2_C, Ws)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _feedforward(self, X, Ws):\n",
    "        \"\"\"Compute feedforward step\n",
    "        \"\"\"\n",
    "        As = [None]*(self.n_hidden_layers+1)\n",
    "        Zs = [None]*self.n_hidden_layers\n",
    "        for i in range(len(As)):\n",
    "            if i==0:\n",
    "                As[0] = self._add_bias_unit(X, how='column')\n",
    "                As[0] = As[0].T\n",
    "            else:\n",
    "                Zs[i-1] = Ws[i-1] @ As[i-1]\n",
    "                As[i] = self._activation(Zs[i-1])\n",
    "                if i!=len(As)-1:\n",
    "                    As[i] = self._add_bias_unit(As[i], how='row')\n",
    "        return As, Zs\n",
    "    \n",
    "    def _get_gradient(self, As, Zs, Y_enc, Ws):\n",
    "        if self.cost_function=='quadratic':\n",
    "            return self._quad_gradient(As,Zs,Y_enc,Ws)\n",
    "        elif self.cost_function=='cross':\n",
    "            return self._cross_gradient(As,Zs,Y_enc,Ws)\n",
    "    \n",
    "    def _quad_gradient(self, As, Zs, Y_enc, Ws):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        Vs = [None]*self.n_hidden_layers\n",
    "        grads = [None]*self.n_hidden_layers\n",
    "        for i in range(self.n_hidden_layers,0,-1):\n",
    "            if i==self.n_hidden_layers:\n",
    "                Vs[i-1] = -2*(Y_enc-As[i])*As[i]*(1-As[i])\n",
    "                grads[i-1] = Vs[i-1] @ As[i-1].T\n",
    "            elif self.n_hidden_layers > 2 and i <= self.n_hidden_layers - 2 and i > 0:\n",
    "                if self.activation_method == 'sigmoid':\n",
    "                    Vs[i-1] = As[i]*(1-As[i])*(Ws[i].T @ Vs[i][1:,:])\n",
    "                elif self.activation_method == 'linear':\n",
    "                    Vs[i-1] = (Ws[i].T @ Vs[i][1:,:])\n",
    "                grads[i-1] = Vs[i-1][1:,:] @ As[i-1].T\n",
    "            else:\n",
    "                if self.activation_method == 'sigmoid':\n",
    "                    Vs[i-1] = As[i]*(1-As[i])*(Ws[i].T @ Vs[i])\n",
    "                elif self.activation_method == 'linear':\n",
    "                    Vs[i-1] = (Ws[i].T @ Vs[i])\n",
    "                grads[i-1] = Vs[i-1][1:,:] @ As[i-1].T\n",
    "        \n",
    "        return grads\n",
    "\n",
    "    \n",
    "    def _cross_gradient(self, As, Zs, Y_enc, Ws):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        \n",
    "        Vs = [None]*self.n_hidden_layers\n",
    "        grads = [None]*self.n_hidden_layers\n",
    "        for i in range(self.n_hidden_layers,0,-1):\n",
    "            if i==self.n_hidden_layers:\n",
    "                Vs[i-1] = As[i]-Y_enc\n",
    "                grads[i-1] = Vs[i-1] @ As[i-1].T\n",
    "            elif self.n_hidden_layers > 2 and i <= self.n_hidden_layers - 2 and i > 0:\n",
    "                if self.activation_method == 'sigmoid':\n",
    "                    Vs[i-1] = As[i]*(1-As[i])*(Ws[i].T @ Vs[i][1:,:])\n",
    "                elif self.activation_method == 'linear':\n",
    "                    Vs[i-1] = (Ws[i].T @ Vs[i][1:,:])\n",
    "                grads[i-1] = Vs[i-1][1:,:] @ As[i-1].T\n",
    "            else:\n",
    "                if self.activation_method == 'sigmoid':\n",
    "                    Vs[i-1] = As[i]*(1-As[i])*(Ws[i].T @ Vs[i])\n",
    "                elif self.activation_method == 'linear':\n",
    "                    Vs[i-1] = (Ws[i].T @ Vs[i])\n",
    "                grads[i-1] = Vs[i-1][1:,:] @ As[i-1].T\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        As,_ = self._feedforward(X, self.Ws)\n",
    "        y_pred = np.argmax(As[-1], axis=0)\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.Ws = self._initialize_weights()\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            # feedforward all instances\n",
    "            As, Zs = self._feedforward(X_data,self.Ws)\n",
    "            \n",
    "            cost = self._cost(As[-1],Y_enc,self.Ws)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            grads = self._get_gradient(As=As, Zs=Zs, Y_enc=Y_enc,\n",
    "                                              Ws=self.Ws)\n",
    "\n",
    "            for i in range(len(self.Ws)):\n",
    "                self.Ws[i] -= self.eta * grads[i]\n",
    "            \n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(n_hidden=50, \n",
    "              n_hidden_layers=4,\n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              epochs=500, # iterations\n",
    "              eta=0.001,  # learning rate\n",
    "              random_state=1,\n",
    "              cost_function='quadratic',\n",
    "              activation_method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 500/500"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.6791907514450867\n",
      "Wall time: 2.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "nn = NLayerPerceptron(**params)\n",
    "nn.fit(X_train, y_train, print_progress=10)\n",
    "yhat = nn.predict(X_test)\n",
    "print('Test acc:',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 500/500"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.6791907514450867\n",
      "Wall time: 2.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "nn = NLayerPerceptron(**params)\n",
    "nn.fit(X_train, y_train, print_progress=10)\n",
    "yhat = nn.predict(X_test)\n",
    "print('Test acc:',accuracy_score(y_test,yhat))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
